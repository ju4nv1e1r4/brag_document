# Dezembro

## *05/12/2025*

### üöÄ Otimiza√ß√£o de Engenharia de Prompt e Arquitetura Neuro-Simb√≥lica para Datas

**Contexto (O Problema):**
O agente de IA sofria com "alucina√ß√µes aritm√©ticas" ao calcular datas relativas e falhava na interpreta√ß√£o do contexto de calend√°rio.
Inicialmente, o contexto era injetado via um **Grid ASCII** (simulando um calend√°rio visual). Identifiquei que isso gerava ru√≠do na tokeniza√ß√£o: o modelo perdia a refer√™ncia espacial devido √† quebra de tokens de espa√ßamento e quebras de linha, resultando em agendamentos errados.

**A Solu√ß√£o (O que eu fiz):**
Arquitetei uma solu√ß√£o h√≠brida (Neuro-Simb√≥lica) e refatorei a inje√ß√£o de contexto:

  * **Otimiza√ß√£o de Contexto (Attention-Friendly):** Substitu√≠ o Grid ASCII por uma estrutura linear de strings sem√¢nticas (ex: `"- Sexta-feira [HOJE]: 09/12"`). Isso eliminou a necessidade do modelo ter racioc√≠nio espacial visual, entregando a informa√ß√£o mastigada para o mecanismo de *Attention*.
  * **Pipeline de Sanitiza√ß√£o:** Implementei um *Tool Calling* onde um LLM (`gpt-4o-mini`) atua apenas como normalizador de linguagem natural (convertendo g√≠rias como "fds" ou "pr√≥xima sexta"), removendo a responsabilidade l√≥gica dele.
  * **Motor Determin√≠stico:** Desenvolvi o `DateEngine` em Python (com `dateparser` e `Regex`) para realizar o c√°lculo exato da data baseada na entrada sanitizada, garantindo precis√£o de 100% em opera√ß√µes de calend√°rio.

**Resultados & Impacto:**

  * **Redu√ß√£o dr√°stica de alucina√ß√£o:** A mudan√ßa do formato ASCII para Lista Sem√¢ntica eliminou erros de interpreta√ß√£o de dias da semana.
  * **Efici√™ncia de Tokens:** A nova representa√ß√£o textual √© mais densa, consumindo menos tokens por requisi√ß√£o (redu√ß√£o de custo e lat√™ncia).
  * **Experi√™ncia do Usu√°rio:** O sistema agora compreende perfeitamente refer√™ncias relativas complexas ("s√°bado sem ser este, o outro") e contextos brasileiros.


**Pr√≥ximo Passo:**
Para deixar isso ainda mais profissional, voc√™ pode mencionar se usou alguma biblioteca para validar essa redu√ß√£o de tokens (como o `tiktoken`) ou se foi emp√≠rico. Quer que eu escreva um script r√°pido usando `tiktoken` para voc√™ comparar quantos tokens o Grid ASCII gastava vs a sua nova Lista, para adicionar um n√∫mero exato no "Resultados"?
## *02/12/2025*
#### Arquitetura de Infer√™ncia de Alta Performance (Worker N+1)

**Contexto:**
Necessidade de servir m√∫ltiplos modelos de NLP (NER, Zero-Shot) em produ√ß√£o com baixa lat√™ncia e custo reduzido, sem depender de GPUs caras.
A arquitetura anterior (ou a falta de uma) dificultava a gest√£o de vers√µes e o scaling horizontal.

**A Solu√ß√£o T√©cnica:**
Desenvolvi um **Worker de Infer√™ncia Unificado** altamente otimizado para CPU.
* **Engine:** Utiliza√ß√£o de **ONNX Runtime + Tokenizers**, garantindo infer√™ncias na ordem de milissegundos em hardware padr√£o.
* **Initialization Strategy:** Implementei carregamento de modelos no *startup* da aplica√ß√£o de forma **ass√≠ncrona**. Isso impede o bloqueio do Event Loop durante a inicializa√ß√£o dos workers e elimina o "Cold Start" para o usu√°rio final.
* **Resource Management:** Sistema de cache local em **Volume Docker**. O download do GCS ocorre apenas na atualiza√ß√£o, reduzindo tr√°fego de rede e tempo de boot.
* **Model Lifecycle (CLI):** Criei uma ferramenta CLI em Python para gerenciar o ciclo de vida dos modelos (download, upload para GCS e atualiza√ß√£o de refer√™ncias), preparando o terreno para um futuro pipeline de GitOps.

**Impacto & Resultados:**
* **Efici√™ncia:** Capacidade de rodar m√∫ltiplos modelos (N+1) em um √∫nico cont√™iner com consumo de mem√≥ria compartilhado.
* **UX:** Lat√™ncia de infer√™ncia impercept√≠vel para o usu√°rio final devido ao pr√©-carregamento e quantiza√ß√£o.
* **Developer Experience:** Processo de atualiza√ß√£o de modelos desacoplado do c√≥digo da aplica√ß√£o via CLI.

**Stack:** Python, Redis, ONNX Runtime, Docker Volumes, Google Cloud Storage.

