# AVISO IMPORTANTE

Este documento trata de informa√ß√µes ver√≠dicas mas o texto foi revisado e corrigido por um LLM, basicamente pra melhorar a escrita. 

**Dica:** Use IA pra te ajudar a ir mais rapido onde voc√™ sabe que pode ir.

# Dezembro

## *05/12/2025*

### Otimiza√ß√£o de Engenharia de Prompt e Arquitetura Neuro-Simb√≥lica para Datas

**Contexto (O Problema):**
O agente de IA sofria com "alucina√ß√µes aritm√©ticas" ao calcular datas relativas e falhava na interpreta√ß√£o do contexto de calend√°rio.
Inicialmente, o contexto era injetado via um **Grid ASCII** (simulando um calend√°rio visual). Identifiquei que isso gerava ru√≠do na tokeniza√ß√£o: o modelo perdia a refer√™ncia espacial devido √† quebra de tokens de espa√ßamento e quebras de linha, resultando em agendamentos errados.

**A Solu√ß√£o (O que eu fiz):**
Arquitetei uma solu√ß√£o h√≠brida (Neuro-Simb√≥lica) e refatorei a inje√ß√£o de contexto:

  * **Otimiza√ß√£o de Contexto (Attention-Friendly):** Substitu√≠ o Grid ASCII por uma estrutura linear de strings sem√¢nticas (ex: `"- Sexta-feira [HOJE]: 09/12"`). Isso eliminou a necessidade do modelo ter racioc√≠nio espacial visual, entregando a informa√ß√£o mastigada para o mecanismo de *Attention*.
  * **Pipeline de Sanitiza√ß√£o:** Implementei um *Tool Calling* onde um LLM (`gpt-4o-mini`) atua apenas como normalizador de linguagem natural (convertendo g√≠rias como "fds" ou "pr√≥xima sexta"), removendo a responsabilidade l√≥gica dele.
  * **Motor Determin√≠stico:** Desenvolvi o `DateEngine` em Python (com `dateparser` e `Regex`) para realizar o c√°lculo exato da data baseada na entrada sanitizada, garantindo precis√£o de 100% em opera√ß√µes de calend√°rio.

**Resultados & Impacto:**

  * **Redu√ß√£o dr√°stica de alucina√ß√£o:** A mudan√ßa do formato ASCII para Lista Sem√¢ntica eliminou erros de interpreta√ß√£o de dias da semana.
  * **Efici√™ncia de Tokens:** A nova representa√ß√£o textual √© mais densa, consumindo menos tokens por requisi√ß√£o (redu√ß√£o de custo e lat√™ncia).
  * **Experi√™ncia do Usu√°rio:** O sistema agora compreende perfeitamente refer√™ncias relativas complexas ("s√°bado sem ser este, o outro") e contextos brasileiros.

## *02/12/2025*

#### Arquitetura de Infer√™ncia de Alta Performance (Worker N+1)

**Contexto:**
Necessidade de servir m√∫ltiplos modelos de NLP (NER, Zero-Shot) em produ√ß√£o com baixa lat√™ncia e custo reduzido, sem depender de GPUs caras.
A arquitetura anterior (ou a falta de uma) dificultava a gest√£o de vers√µes e o scaling horizontal.

**A Solu√ß√£o T√©cnica:**
Desenvolvi um **Worker de Infer√™ncia Unificado** altamente otimizado para CPU.
* **Engine:** Utiliza√ß√£o de **ONNX Runtime + Tokenizers**, garantindo infer√™ncias na ordem de milissegundos em hardware padr√£o.
* **Initialization Strategy:** Implementei carregamento de modelos no *startup* da aplica√ß√£o de forma **ass√≠ncrona**. Isso impede o bloqueio do Event Loop durante a inicializa√ß√£o dos workers e elimina o "Cold Start" para o usu√°rio final.
* **Resource Management:** Sistema de cache local em **Volume Docker**. O download do GCS ocorre apenas na atualiza√ß√£o, reduzindo tr√°fego de rede e tempo de boot.
* **Model Lifecycle (CLI):** Criei uma ferramenta CLI em Python para gerenciar o ciclo de vida dos modelos (download, upload para GCS e atualiza√ß√£o de refer√™ncias), preparando o terreno para um futuro pipeline de GitOps.

**Impacto & Resultados:**
* **Efici√™ncia:** Capacidade de rodar m√∫ltiplos modelos (N+1) em um √∫nico cont√™iner com consumo de mem√≥ria compartilhado.
* **UX:** Lat√™ncia de infer√™ncia impercept√≠vel para o usu√°rio final devido ao pr√©-carregamento e quantiza√ß√£o.
* **Developer Experience:** Processo de atualiza√ß√£o de modelos desacoplado do c√≥digo da aplica√ß√£o via CLI.

**Stack:** Python, Redis, ONNX Runtime, Docker Volumes, Google Cloud Storage.

## *10/12/2025*

## üöÄ Otimiza√ß√£o de Contexto em LLM com Inje√ß√£o Din√¢mica via Redis

**Contexto (O Problema):**
O m√≥dulo de agendamento sofria com instabilidade nas respostas da LLM (alucina√ß√µes) devido √† polui√ß√£o do contexto. Ao retornar listas extensas de hor√°rios diretamente no hist√≥rico da conversa, exced√≠amos a janela de aten√ß√£o √∫til do modelo, fazendo com que ele se confundisse entre modalidades ou inventasse hor√°rios. Al√©m disso, o fluxo se perdia quando dados intermedi√°rios (como CPF ou data de nascimento) eram solicitados, exigindo que o usu√°rio reiniciasse a inten√ß√£o.

**A Solu√ß√£o (O que eu fiz):**
Arquitetou-se um padr√£o de **Dynamic Context Injection** (Inje√ß√£o Din√¢mica de Contexto) para desacoplar a recupera√ß√£o de dados da apresenta√ß√£o para a IA:

* **Gerenciamento de Estado com Redis:** Implementa√ß√£o de uma estrat√©gia de cache com TTL (10 min) para armazenar estados transientes da agenda (`ctx:agenda:today`, `ctx:agenda:instruction`).
* **Higiene de Contexto:** Cria√ß√£o de l√≥gica para limpar chaves conflitantes (ex: apagar hor√°rios quando o foco muda para escolha de modalidades), garantindo que a LLM tenha acesso apenas √† "verdade" do momento atual.
* **Pipeline de Inje√ß√£o no System Prompt:** Desenvolvimento do m√©todo `_get_dynamic_agenda_context` que intercepta a constru√ß√£o do prompt e injeta os dados "quentes" do Redis diretamente nas instru√ß√µes do sistema, mantendo o hist√≥rico de conversa do usu√°rio limpo.
* **Resili√™ncia de Fluxo (State Recovery):** Implementa√ß√£o de verifica√ß√£o de "fluxo pendente" (ex: `was_booking_flow`). Se o sistema precisa interromper o agendamento para pedir um dado (ex: Data de Nascimento), ele agora "lembra" da inten√ß√£o original e retoma a busca de aulas automaticamente ap√≥s a inser√ß√£o do dado.

**Resultados & Impacto:**
* **Redu√ß√£o de Alucina√ß√µes:** A segrega√ß√£o entre instru√ß√µes de fluxo e dados brutos aumentou significativamente a precis√£o do modelo ao sugerir hor√°rios.
* **Otimiza√ß√£o de Tokens:** Envio sob demanda de informa√ß√µes cr√≠ticas apenas no System Prompt, economizando tokens de input no hist√≥rico longo.
* **UX Mais Fluida:** O usu√°rio n√£o precisa mais repetir o comando "quero agendar" ap√≥s fornecer um dado cadastral faltante; o sistema reconecta o fluxo automaticamente.
* **Melhoria de Observabilidade:** Logs estruturados indicando quando o contexto din√¢mico foi injetado ou quando falhou (tratamento de erro robusto no acesso ao Redis).
